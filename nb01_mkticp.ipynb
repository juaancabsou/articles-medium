{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Notebook: Marketing ICP generation with clustering\n",
    "# Unsupervised Algorithms applications\n",
    "#\n",
    "# (Â©) 2022 Juan Antonio Cabeza Sousa, Spain\n",
    "# email juaancabsou@gmail.com\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/Juan/Documents/GitHub/medium\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from etl01_basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This project aims to apply unsupervised algorithms as clustering in order to classify different types of customers without labels.\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "Customer Segmentation or Customer Personality Analysis is about creating different groups or segment of customers based on their behaviors. This segmentation aims to help businesses to understand their customers allowing them to:\n",
    "\n",
    "- Identify pain points and concerns in customers\n",
    "- Modify products and behaviors to specific needs\n",
    "- Improve marketing campaigns and targeting\n",
    "\n",
    "**The goal of this notebook**\n",
    "\n",
    "This notebook aims to classify the different customers from *marketing_campaign.csv* in different segments with the goal of improving marketing campaigns in the future.\n",
    "\n",
    "## The data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/Juan/Documents/GitHub/medium/data/\"\n",
    "PATH_DATA = PATH + \"/data/\"\n",
    "data = read_data(PATH_DATA, \"marketing_campaign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickstats_df(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "This stage covers the first analysis of the data. The goal of this is to understand better the initial data that we have and doing an initial cleaning like filling empty values, removing outliers, drop useless information...so on so forth.\n",
    "\n",
    "As a first steps, it is possible to see that:\n",
    "- We don\"t have the age of the user, instead we have the `YearBirth`\n",
    "- Column `Dt_Customer` is not parsed as DateTime. Also, this column represents the date of customer\"s enrollment with the company but we don\"t have as a number the seniority of a client. We\"ll let that to the feature engineering stage\n",
    "- Column `Income` has 1.07% of empty values. As this percentage represents just 24 instances at the moment, we\"ll remove it for now.\n",
    "- We have several columns with categorical information. We need to check the proportion of those categories and also to encode them as numeric.\n",
    "    - Education is quite specific currently so we\"ll create just 3 categories in order to simplify the data.\n",
    "    - In this case, it is possible to see that there are a few values in `Marital_State` column that should be categorised as *Alone*. Also the idea is to simplify categories here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean datetime\n",
    "data[\"Dt_Customer\"] = pd.to_datetime(data[\"Dt_Customer\"], format=\"%d-%m-%Y\")\n",
    "# For this research I will remove the 1% of NAs that we have\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Marital Status\n",
    "data[\"Education\"] = data[\"Education\"].apply(lambda x: \"NoGraduate\" if x in [\"Basic\", \"2n Cycle\"] else \"Graduate\")\n",
    "data[\"Marital_Status\"] = data[\"Marital_Status\"].apply(lambda x: \"Alone\" if x in [\"Single\", \"Absurd\", \"YOLO\", \"Widow\"] else (\"Partner\" if x in [\"Married\", \"Together\"] else x))\n",
    "\n",
    "display(pd_categories_trans(data, \"Education\"))\n",
    "display(pd_categories_trans(data, \"Marital_Status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing\n",
    "## 3.1 - Feature Engineering\n",
    "\n",
    "After have cleaned the data, we can focus on generating new features about these customers. In order to do that, we will:\n",
    "\n",
    "- Create new feature `Age` from the `Year_Birth` column.\n",
    "- Create new feature `Seniority` from the `Dt_customer`.\n",
    "- Create new feature `Partner` in order to replace `Marital_Status`. We just want to know if the customer has a partner or not.\n",
    "- Create new feature `Children` for replacing `Kidhome` and `Teenhome`\n",
    "- Create new feature `FamilySize` in order to know how many people live with the client.\n",
    "- Create new feature `IsParent` from previous feature.\n",
    "- Create new feature `Bill` as the summatory of amounts spent by the customer in the different categories over the 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Age from the Year (The data was created in 2021)\n",
    "data[\"Age\"] = 2021 - data[\"Year_Birth\"]\n",
    "\n",
    "# In order to calculate the seniority, use the max date in the database as last record\n",
    "max_record = max(data[\"Dt_Customer\"])\n",
    "data[\"Seniority\"] = (max_record - data[\"Dt_Customer\"]).dt.days\n",
    "\n",
    "\n",
    "# Family Features\n",
    "data[\"Partner\"] = data[\"Marital_Status\"].apply(lambda x: 1 if x ==\"Partner\" else 0)\n",
    "data[\"Children\"] = data[\"Kidhome\"] + data[\"Teenhome\"]\n",
    "data[\"FamilySize\"] = data[\"Children\"]+data[\"Partner\"] + 1\n",
    "data[\"IsParent\"] = data[\"Children\"].apply(lambda x: 1 if x>=1 else 0)\n",
    "\n",
    "# Bill feature\n",
    "data[\"Bill\"] = data[\"MntWines\"]+ data[\"MntFruits\"]+ data[\"MntMeatProducts\"]+ data[\"MntFishProducts\"]+ data[\"MntSweetProducts\"]+ data[\"MntGoldProds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Useless columns\n",
    "billcols = [\"MntWines\", \"MntFruits\", \"MntMeatProducts\", \"MntFishProducts\", \"MntSweetProducts\", \"MntGoldProds\"]\n",
    "mkt_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4','AcceptedCmp5']\n",
    "othercols = ['Complain', 'Response']\n",
    "\n",
    "data.drop([\n",
    "    \"Dt_Customer\",\n",
    "    \"Marital_Status\",\n",
    "    \"Z_CostContact\",\n",
    "    \"Z_Revenue\",\n",
    "    \"Year_Birth\",\n",
    "    \"Kidhome\",\n",
    "    \"Teenhome\",\n",
    "    \"ID\"\n",
    "], axis=1, inplace=True)\n",
    "\n",
    "data.drop(billcols, axis=1, inplace=True)\n",
    "data.drop(othercols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data Checking\n",
    "Once that we have finished generating features it is time to check the final dataset information. In order to do that, I usually check the data with 3 different approaches:\n",
    "1. Data completion: How many empty values do we have after the feature engineering?\n",
    "2. Data outliers: How many outliers do we have? And of course, fixing the outliers as well\n",
    "3. Data coherence: Does each feature contain coherent with the rest of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see that there are some incoherence in our data as:\n",
    "- Ages above 128 years old.\n",
    "- Mean in `Income` is quite high and might contain outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking outliers\n",
    "colsplot = [\"Income\", \"Recency\", \"Seniority\", \"Age\", \"Bill\", \"IsParent\"]\n",
    "\n",
    "colors = ['#fbbc04', '#999999']\n",
    "\n",
    "sns.set(rc={\"axes.facecolor\":\"#F3F3F3\",\"figure.facecolor\":\"#F3F3F3\"})\n",
    "sns.pairplot(data[colsplot], hue= \"IsParent\",palette= (colors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing outliers\n",
    "df = data[\n",
    "        (data[\"Age\"]<90) &\n",
    "        (data[\"Income\"]<600000)\n",
    "    ].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is cleaned from outliers and weird values, it is time for preprocessing the dataset for feeding the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Preprocessing stage\n",
    "\n",
    "Before feeding the models with our data we need to preprocess a little bit the current dataset. The preprocessing stage has most of the time the **following steps**:\n",
    "\n",
    "1. **Label encoding**: To encode categorical features, that means from string to a cardinal number for representing the category.\n",
    "2. **Scaling features**: Scaled data makes it easy for a model to learn and understand the problem as the algorithms can calculate the distance between the data points easier for making better inferences out of the data.\n",
    "3. **Dimensionality Reduction** As the current dataset have too many fators for doing the classification, algorithms might struggle for doing the calculations. There are several features that are correlated and therefore they are redundant. Using dimensionality reduction we'll keep just the features that are worth to consider for the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "LE = LabelEncoder()\n",
    "\n",
    "df['Education'] = df[['Education']].apply(LE.fit_transform)\n",
    "le_name_mapping = dict(zip(LE.classes_, LE.transform(LE.classes_)))\n",
    "print(le_name_mapping)\n",
    "\n",
    "df['Partner'] = df[['Partner']].apply(LE.fit_transform)\n",
    "\n",
    "le_name_mapping = dict(zip(LE.classes_, LE.transform(LE.classes_)))\n",
    "print(le_name_mapping)\n",
    "\n",
    "for i in range(1,6):\n",
    "    col = 'AcceptedCmp'+ str(i)\n",
    "    df[col] = df[[col]].apply(LE.fit_transform)\n",
    "\n",
    "\n",
    "\n",
    "list(LE.classes_)\n",
    "list(LE.inverse_transform([0,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler\n",
    "data_scaler = df.drop(mkt_cols, axis=1)\n",
    "SC = StandardScaler()\n",
    "SC.fit(data_scaler)\n",
    "SC_values = SC.transform(data_scaler)\n",
    "SC_header = data_scaler.columns\n",
    "SC_df = pd.DataFrame(SC_values, columns=SC_header)\n",
    "SC_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dimensionality reduction with PCA\n",
    "\n",
    "As mentioned before, one of the main steps in the preprocessing is to reduce the number of features of the dataset in order to keep just the features that are not redundant. With this step we are aiming to:\n",
    "1. To reduce the dataset size\n",
    "2. To increase interpretability and features managing\n",
    "3. To minimize the loss of information\n",
    "\n",
    " In order to do this I will use one of the main algorithms that allows to reduce datasets in an easy way, the *Principal Component Analysis or PCA*.\n",
    "\n",
    " For interpretability reasons, I will reduce the dimensionality to 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dimensionality Reduction\n",
    "COMP = 3\n",
    "PCA = PCA(n_components=COMP)\n",
    "PCA.fit(SC_df)\n",
    "PCA_values = PCA.transform(SC_df)\n",
    "PCA_header = ['col_'+str(i+1) for i in range(0,COMP)]\n",
    "\n",
    "PCA_df = pd.DataFrame(PCA_values, columns=PCA_header)\n",
    "PCA_df.head(2)\n",
    "\n",
    "\n",
    "x = PCA_df[\"col_1\"]\n",
    "y = PCA_df[\"col_2\"]\n",
    "z = PCA_df[\"col_3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering the data\n",
    "\n",
    "Once that the preprocessing stage is finished, we can feed our data to the algorithm. In this case, as we don't have labels in our data to predict, I will use Agglomerative Clustering algorithm for grouping the customers. This is a hierarchical clustering method that involves merging examples until the desired number of clusters is achieved.\n",
    "\n",
    "In order to do that, it is necessary to perform several steps:\n",
    "1. To find the number of clusters using the Elbow Method\n",
    "2. To apply the Agglomerative Clustering algorithm\n",
    "3. Plot the clusters in order to analyse them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "elbow = KElbowVisualizer(KMeans(), k=10)\n",
    "elbow.fit(PCA_df)\n",
    "elbow.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the elbow is in 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering\n",
    "# We make a copy of the dataset without preprocessing for plotting next values with new clusters\n",
    "\n",
    "df_cluster = df.copy() \n",
    "pred = PCA_df.copy()\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=4)\n",
    "yhat = ac.fit_predict(pred)\n",
    "\n",
    "df_cluster['cluster'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from matplotlib import colors\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = plt.subplot(projection='3d')\n",
    "cmap = colors.ListedColormap([\"#fbbc04\", \"#999999\", \"#134f5c\", \"#f5959c\", \"#57bb8a\", \"#9900ff\"])\n",
    "\n",
    "ax.scatter(x, y, z, s=40, c=df_cluster[\"cluster\"], marker='o', cmap = cmap )\n",
    "ax.set_title(\"3D Clusters\")\n",
    "plt.show()\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation\n",
    "\n",
    "As this is an unsupervised algorithm and we don't have labels for evaluating the results, it is necessary to understand the output and analysing the cluster in order to check that the patterns make sense.\n",
    "\n",
    "There are several approaches for doing that:\n",
    "1. Clustering distribution + boxplot\n",
    "2. Clustering against the main interesting feature: Income and bill and also age and bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check clustering distribution, 4 clusters -> 4 colors\n",
    "clusters = df_cluster.cluster.value_counts().reset_index().rename(columns={'index':'cluster', 'cluster':'customers'})\n",
    "display(clusters)\n",
    "\n",
    "colors = [\"#fbbc04\", \"#999999\", \"#134f5c\", \"#f5959c\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize = (15,5))\n",
    "\n",
    "ax1 = sns.barplot(x='cluster', y='customers', data=clusters, palette=colors,  ax=axs[0])\n",
    "ax1.set_title('Clusters distribution')\n",
    "\n",
    "ax2 = sns.boxenplot(x=df_cluster[\"cluster\"], y=df_cluster[\"Bill\"], palette=colors,  ax=axs[1])\n",
    "ax2.set_title('Clusters boxplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different groups clustered when we plot `Bill` vs `Income` are:\n",
    "- Group 0 (yellow): High Bill / Avg Income\n",
    "- Group 1 (grey): High Bill / High Income\n",
    "- Group 2 (blue): Low Bill / Low Income\n",
    "- Group 3 (pink): Low Bill / Avg(almost high) Income\n",
    "\n",
    "The different groups clustered when we plot `Income` vs `Age` are:\n",
    "- Group 0 (yellow): Above 40-50 / Avg-high Income\n",
    "- Group 1 (grey): All ages / Customers with high income\n",
    "- Group 2 (blue): Below 40-50 / Low Income\n",
    "- Group 3 (pink): Above 50 / Avg-Low Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize = (15,5))\n",
    "\n",
    "ax1 = sns.scatterplot(x='Bill', y='Income', data=df_cluster, hue=df_cluster['cluster'], palette=colors, ax=axs[0])\n",
    "ax1.set_title('Bill vs Income per cluster')\n",
    "\n",
    "ax2 = sns.scatterplot(x='Income', y='Age', data=df_cluster, hue=df_cluster['cluster'], palette=colors, ax=axs[1])\n",
    "ax2.set_title('Bill vs Income per cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look just the the main clusters 0 and 1 it is possible to see better the difference between the 2 types of main customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_main = df_cluster[df_cluster.cluster.isin([0,1])].copy()\n",
    "fig, axs = plt.subplots(ncols=2, figsize = (15,5))\n",
    "\n",
    "ax1 = sns.scatterplot(x='Bill', y='Income', data=df_cluster_main, hue=df_cluster_main['cluster'], palette=colors[0:2], ax=axs[0])\n",
    "ax1.set_title('Bill vs Income. Cluster 0 and 1')\n",
    "\n",
    "ax2 = sns.scatterplot(x='Income', y='Age', data=df_cluster_main, hue=df_cluster_main['cluster'], palette=colors[0:2], ax=axs[1])\n",
    "ax2.set_title('Bill vs Income. Cluster 0 and 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Marketing campaigns - Cluster Analysis\n",
    "\n",
    "Now that we have identified the clusters, let's take a look at the previous campaigns using the `AcceptedCmpX` columns (being X the number of the campaign).\n",
    "\n",
    "- It is possible to see that the marketing campagins aren't too effective. Most of the customers just accepted 0 campaigns.\n",
    "- Also, althogh there are 5 different campaigns, no one has reached the fifth one.\n",
    "- It is clear that it is necessary to put some course of action in order to improve the performance of the campaigns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster['total_camp'] = df_cluster[\"AcceptedCmp1\"] + df_cluster[\"AcceptedCmp2\"] + df_cluster[\"AcceptedCmp3\"] + df_cluster[\"AcceptedCmp4\"] + df_cluster[\"AcceptedCmp5\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "ax1 = sns.countplot(x=df_cluster[\"total_camp\"], hue=df_cluster['cluster'], palette=colors)\n",
    "ax1.set_title('Campaigns accepted per cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Profiling with clustering\n",
    "\n",
    "Last step of our research is to create several profiles using the clustering results. This will help us to identify better the customers' habit for buying in the store. The **goal here is to find out which is our top profile customer** in order to **maximize our sellings** or in other words, the money spent per customer.\n",
    "\n",
    "In order to do that, let's take a look at the main descriptive attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = [ \"Children\", \"Seniority\", \"Age\", \"FamilySize\", \"IsParent\", \"Education\", \"Partner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in desc:\n",
    "    plt.figure()\n",
    "    sns.jointplot(x=df_cluster[i], y=df_cluster[\"Bill\"], hue =df_cluster[\"cluster\"], kind=\"kde\", palette=colors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('kaggle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "451b701d82ab33434d8672b9edd9255413ffe2ad42ba5d7999c259d9e8746926"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
